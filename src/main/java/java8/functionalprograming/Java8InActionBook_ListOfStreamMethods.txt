Building Stream

    Empty stream
        Stream<String> emptyStream = Stream.empty()

    From values
        Stream<String> stream = Stream.of("s1","s2","s3")

    From array
        Stream<String> stream = Arrays.stream(arrayOfString)

    From the file
        Stream<String> lines = Files.lines(Paths.get("./MyJavaProject/src/java8/data.txt"), Charset.defaultCharset());

    From collection
        Stream<Integer> stream = list.stream();

    From iterator
        Iterator<Integer> iterator = list.iterator();

        From Iterator, you need to create spliterator and using spliterator, you can create a stream.

        Spliterator<Integer> spliterator = Spliterators.spliteratorUnknownSize(sourceIterator, Spliterator.IMMUTABLE | Spliterator.NONNULL | Spliterator.DISTINCT | Spliterator.ORDERED | Spliterator.SORTED |Spliterator.SIZED | Spliterator.SUBSIZED);

        Stream<Integer> targetStream = StreamSupport.stream(spliterator, true);
    

Creating Infinite Streams

    Stream.iterate(...) and Stream.generate(...)

    Stream.iterate(0, n -> n + 2)  // it is just like reduce method going in infinite loop
    .limit(10);

    intStream.range(0, 10).map(n -> n+2) ---
    using range is better than iterate because iterate works on objects, so if you use iterate over ints, there will a cost of unboxing for n+2 like operations and then converting back to Integer(boxing) for creating Stream<Integer>
    Use iterate for real objects(not wrappers of primitives).


    IntStream ones = IntStream.generate(() -> 1);   --- This will repeatedly generate 1 infinitely.
    Stream.generate(Math::random).limit(5);

Stateful Operations

    Intermediate Operations

        sorted, distinct - unbound (infinite) state
        skip, limit - bounded state

        Do not use parallel processing on stateful operations. To maintain the state correctly between many threads, it needs some kind of synchronization, which is expensive.

        numbers.parallelStream().distinct()... ---- try to avoid
        numbers.stream().distinct().parallelStream().... ---- good

    Terminal Operations

        reduce, sumIteratively, min, max etc are stateful, but their state is very small and bounded.

count
    stream.count() gives total number of elements in a stream.

map and flatMap

    map(function), flatMap(function)

Filtering operations

    filter(predicate)
    anyMatch(predicate), allMatch, noneMatch

    Optional findAny(), findFirst()

Operations taking Consumer as a parameter

    void forEach(consumer)
    void forEachOrdered(consumer)
    Stream peek(consumer) --- peek returns a stream
        Stream.of("AAA","BBB","CCC").parallel().forEach(s->System.out.print(s)); // CCCAAABBB order is not maintained in parallel processing
        Stream.of("AAA","BBB","CCC").parallel().forEachOrdered(s->System.out.print(s));// AAABBBCCC order is always maintained in parallel processing




reduce(initial value/seed/identity, BinaryOperator accumulator, BinaryOperator combiner) --- returns value
reduce(initial value/seed/identity, BinaryOperator accumulator) --- returns value  --- same as reduce(identity,accumulator,accumulator) accumulator function is used for combiner also
reduce(BinaryOperator accumulator)  --- returns Optional --- same as reduce(Optional.empty(),accumulator,accumulator)


sum(), min(), max()

    They are variant of reduce() only. They can be used on IntStream/LongStream/DoubleStream etc.

    Integer result = numbers.stream().sum()
    is same as
    Integer result = numbers.stream().reduce(0, Integer::sum)
    is same as
    Integer result = numbers.stream().reduce(0, (n1,n2) -> n1+n2)


    OptionalInt num = numbers.stream().min()
    is same as
    Optional<Integer> num = numbers.stream().reduce(Math::min) // See it uses reduce(accumulator) method, so it returns Optional


    OptionalInt num = numbers.stream().max()
    is same as
    Optional<Integer> num = numbers.stream().reduce(Math::max) // See it uses reduce(accumulator) method, so it returns Optional

Numeric Streams

    IntStream/LongStream/DoubleStream

        map vs mapToInt

            When summation of two numbers are done, it needs to be done on primitives (ints and not Integers).
            There is a cost of converting Integer to int (Unboxing) for summation and then Boxing again after summation is done.
            If you see below reduce method, that is what exactly would happen.

            Stream<Integer> stream = numbers.stream().map((n) -> n * n).reduce(0, (n1, n2) -> n1+n2);

            To avoid the cost of Unboxing and Boxing during reduce operation for two numbers n1 and n2, you can just do that once before calling reduce.

            IntStream stream = numbers.stream().mapToInt((n) -> n * n).sum()
            Stream<Integer> stream = stream.boxed();

        range(n1,n2), rangeClosed(n1,n2)

            IntStream.range(0,10).filter(.....)
            This is like for(int i=0; i<10; i++)

            IntStream.rangeClosed(0,10).filter(.....)
            This is like for(int i=0; i<=10; i++)


Files utility

    There are some utility methods in Files class that now returns Stream.

    Stream<String> lines = Files.lines(Paths.get("./MyJavaProject/src/java8/data.txt"), Charset.defaultCharset());
    //Stream<String[]> stream = lines.map(line -> line.split(" "));
    Stream<String> stream = lines.flatMap(line -> Arrays.stream(line.split(" ")));
    long uniqueWordCount = stream.distinct().count();

collect method

    collection.stream().collect(Collector)    --- Collector is an interface. This is the most row form. All other syntaxes get converted into this form eventually.
    collection.stream().collect(supplier, accumulator and combiner)
    collection.stream().collect(Collectors utility method)

    Collector<T,A,R> is an interface. It has following methods

        Supplier<A> supplier()
        BiConsumer<A, T> accumulator()
        BinaryOperator<A> combiner()
        Function<A, R> finisher()
        Set<Characteristics> characteristics()
        // This method can create an instance of CollectorImpl
        Collector<T, A, R> of(Supplier<A> supplier,
                              BiConsumer<A, T> accumulator,
                              BinaryOperator<A> combiner, --- useful for parallelism. accumulated result from two threads are combined by a combiner.
                              Function<A, R> finisher, --- combined result can be transformed to some other type by finisher. It is optional.
                              Characteristics... characteristics)


        List collector = integerStream.collect(
                                    ArrayList::new, // same as () -> new ArrayList<>()  --- identity list
                                    List::add, // same as //(identityList,b) -> identityList.add(b)
                                    List::addAll // same as (lists from thread1, list2 from thread2) -> {return list1.addAll(list2);}
                            );
        is same as
        List collector = integerStream.collect(Collectors.toList());

        Internally, it is converted to
        List collector = integerStream.collect(
                                                Collectors.new CollectorImpl<>(ArrayList::new,
                                                                               List::add,
                                                                               (left, right) -> { left.addAll(right); return left; },
                                                                               Collector.Characteristics.IDENTITY_FINISH --- This will create result in a Finisher that just returns the output same as input. I don't see any API that let's you pass Finisher. May be it is just for Java's internal use.
                                                                              )
                                              );
        Collectors Utility Methods

            All utility methods of Collectors returns a Collector that will have a Supplier, Accumulator, Combiner, Finisher
            stream.collect(Collector) method executes this collector

            How collector is executed by a collect method?
                Till stream ends {
                    O identityResult = collector.supplier().get();
                    collector.getAccumulator().apply(streamElement, identityResult); --- accumulator collects stream element into identityResult
                }

                See example in MyStreamReduceCollectGroupingByMappingEtcApi.java


            - Reducing and summarizing
                Collectors.reducing(BinaryOperator) or (identity, Function mapper, BinaryOperator op)
                    It internally creates Collector only, which accepts identity element as Supplier that helps during parallel processing. As you know there is a difference between stream.collect(Collectors.reducing(...) and stream.reduce(...) methods when parallel processing happens.
                Collectors.counting()
                    It uses Collectors.reducing(0L, e -> 1L, Long::sum) internally
                Collectors.minBy
                    It uses Collectors.reducing(BinaryOperator.minBy(comparator)) internally
                Collectors.maxBy
                    It uses Collectors.reducing(BinaryOperator.maxBy(comparator)) internally

                Collectors.summingInt  --- same as stream.mapToInt(...).sum(), which is same as stream.mapToInt(...).reduce(0, Integer::sum), but .collect never uses reduce internally. It always uses Collector interface.
                Collectors.summingLong
                Collectors.summingDouble

                Collectors.averagingInt
                Collectors.averagingLong
                Collectors.averagingDouble

                Collectors.comparingInt
                Collectors.comparingLong
                Collectors.comparingDouble

                Collectors.summerizingInt - It takes ToIntFunction as a parameter that can convert passed object to int. It returns a Collector that has a Finisher IntSummaryStatistics. This Finisher just summarizes everything like sumIteratively, min, max, count etc.

                Collectors.toList() // If you want Collectors to create a new nilList to collect elements.
                Collectors.toCollection(() -> nilList) // if you want to use external nilList to collect elements. Try to avoid using this approach because it can create problems when you use Parallel stream when you try to mutate the same nilList shared between multiple threads. To understand it more, read Chapter 7's 7.1.3 section. Use first approach (toList()) which will create a new nilList for each thread and these lists from all threads will be combined in one at the end.
                Collectors.toSet()
                Collectors.toCollection(() -> set)

                Collectors.joining(), Collectors.joining(delimiter) --- use 'Collectors.joining' instead of '.reduce("", (s1,s2) -> s1+s2)' because Collectors.joining uses StringBuilder to concatenate strings, where as reduce method doesn't.
                    String shortMenu = menu.stream().map(Dish::getName).collect(Collectors.joining());
                    String shortMenu = menu.stream().map(Dish::getName).collect(joining(", "));
                
                
                Collectors.groupingBy

                    Most Raw Syntax:

                        Map<key, value> result =
                        Collectors.groupingBy(Function, ---  to decide key of Map
                                             Supplier, ---  supplier of identity map (default is () -> new HashMap())
                                             Collector) ---  for collecting result as map's value (default is Collectors.toList()). (IMP) it can take any Collector instance returned by any utility method of Collectors class.


					    (IMP) See example in MyStreamReduceCollectGroupingByMappingEtcApi.java

                    Other Syntaxes:

                        Collectors.groupingBy(Function)
                            internally uses
                        Collectors.groupingBy(Function, HashMap::new, Collectors.toList())

                        Collectors.groupingBy(Function, mapping(Function, Collector)) --- mapping returns a Collector

                            people.stream().collect(Collectors.groupingBy(Person::getCity))
                            creates a Map<city, List<Person>>

                            people.stream().collect(Collectors.groupingBy(Person::getCity, Collectors.mapping(Person::getLastName, toSet())));
                            It creates a Map<city, Set<lastname>>
					
				Collectors.mappingBy

					Collectors.mapping(Function mapper, Collector downstream)

						Before calling Collector's accumulator, it transforms the accumulator's input using mapper function.

						List<Integer> collectingAge2 = persons.stream()
															  .collect(ArrayList<Integer>::new,
																		(list, person) -> list.add(person.getAge()), // it is adding person's age 
																		(list1, list2) -> list1.addAll(list2));
						// is same as
						List<Integer> collectingAge1 = persons.stream()
															  .collect(Collectors.mapping(person -> person.getAge(), 
															  							  Collectors.toList())
															  		  );
						

                Collectors.partitioningBy(Predicate, Collector)
                    It is same as groupingBy creating a Map<Boolean, ...>

                Collectors.collectingAndThen(Collector<T,A,R>, Function<R,RR> finisher)
                    performs an additional finishing transformation

Parallel Stream

    Parallel streams work under the hood by employing the fork/join framework introduced in Java 7

    A parallel stream is a stream that splits its elements into multiple chunks, processing each chunk with a different thread. Thus, you can automatically partition the workload of a given operation on all the cores of your multicore processor and keep all of them equally busy.

    You can keep changing from parallel to sequential and vice-versa like below, but....
        stream.parallel() .filter(...).sequential().map(...).parallel().reduce();
    But the last call to parallel or sequential wins and affects the pipeline globally. In this example, the pipeline will be executed in parallel because that’s the last call in the pipeline.


    Configuring the thread pool used by parallel streams

        Parallel streams internally use the default ForkJoinPool, which by default has as many threads as you have processors, as returned by Runtime.getRuntime().availableProcessors().
        But you can change the size of this pool using the system property java.util.concurrent.ForkJoinPool.common.parallelism, as in the following example:

        System.setProperty("java.util.concurrent.ForkJoinPool.common.parallelism", "12");

        This is a global setting, so it will affect all the parallel streams in your code. Conversely, it currently isn’t possible to specify this value for a single parallel stream. In general, having the size of the ForkJoinPool equal to the number of processors on your machine is a meaningful default, and we strongly suggest that you not modify it unless you have a very good reason for doing so.

    When not to use Parallelism?

        - For cases like stream.reduce(...) where identity passed to reduce method is shared by all threads.
          you can use parallelism for stream.collect(Collector) safely.

        - Watch out for boxing. Automatic boxing and unboxing operations can dramatically hurt performance. Java 8 includes primitive streams (IntStream, LongStream, and DoubleStream) to avoid such operations, so use them when possible.
          See the difference between stream.iterate and instream.range

        - Some operations naturally perform worse on a parallel stream than on a sequential stream.
          In particular, operations such as limit and findFirst that rely on the order of the elements are expensive in a parallel stream. For example, findAny will perform better than findFirst because it isn’t constrained to operate in the encounter order. You can always turn an ordered stream into an unordered stream by invoking the method unordered on it. So, for instance, if you need N elements of your stream and you’re not necessarily interested in the first N ones, calling limit on an unordered parallel stream may execute more efficiently than on a stream with an encounter order (for example, when the source is a List).

            e.g.
            IntStream.range(0, n).parallel().filter(...).findFirst();
            parallel() will create bunch of threads and split the elements to those threads.
            filter will run in every thread.
            At the end, each thread will submit its result to findFirst.
            If stream is ordered, findFirst's job becomes more expensive because it literally has to wait for the first thread to finish and check its result before going to next.
            So, to improve the performance, you can either use findAny or parallel().unordered().filter(...).findFirst() in which whichever thread finishes first will be observed for its result.

        - Consider the total computational cost of the pipeline of operations performed by the stream. With N being the number of elements to be processed and Q the approximate cost of processing one of these elements through the stream pipeline, the productRecursively of N*Q gives a rough qualitative estimation of this cost. A higher value for Q implies a better chance of good performance when using a parallel stream.

        - For a small amount of data, choosing a parallel stream is almost never a winning decision. The advantages of processing in parallel only a few elements aren’t enough to compensate for the additional cost introduced by the parallelization process.

        - Take into account how well the data structure underlying the stream decomposes. For instance, an ArrayList can be split much more efficiently than a LinkedList, because the first can be evenly divided without traversing it, as it’s necessary to do with the second. Also, the primitive streams created with the range factory method can be decomposed quickly.

                        Source              Decomposability
                        ------              ---------------
                        ArrayList           Excellent
                        LinkedList          Poor
                        IntStream.range     Excellent
                        Stream.iterate      Poor
                        HashSet             Good
                        TreeSet             Good

            Using ArrayList instead of LinkedList with Parallelization is better.

            Using IntStream.range instead of Stream.iterate is better with Parallelization.
                Stream<T> iterate(final T seed, final UnaryOperator<T> f)
                IntStream range(int startInclusive, int endExclusive)

                Using range is better than iterate because iterate works on objects, so if you use iterate over ints, there will a cost of unboxing for n+2 like operations and then converting back to Integer(boxing) for creating Stream<Integer>
                Use iterate for real objects(not wrappers of primitives).

        - Consider whether a terminal operation has a cheap or expensive deleteRootAndMergeItsLeftAndRight step (for example, the combiner method in a Collector). If this is expensive, then the cost caused by the combination of the partial results generated by each substream can outweigh the performance benefits of a parallel stream.

        - Parallelism should not be used for undbounded stateful operations like stream.distinct, stream.sorted


Power of Laziness

    Chapter 9 of FunctionalProgrammingInJavaBook.java

    1)
        Supplier<Integer> from(int i) {
            return from(i+1); ----- infinite loop
        }

        Any recursion can be converted to lazy call using a Supplier

        Supplier<Integer> from(int i) {
            return () -> from(i + 1); ---- no recursion. Every call is returned back to client and client decides to make another call.
        }

        int i=0;
        while(i < 10) {
            Supplier<Integer> res = from(i++);
            System.out.println(res.get());
        }


        To fulfill the advantage of laziness, sometimes you may have to make class variable also lazy.

        class Cons extends Stream<I> {
            private Cons(I head, Supplier<Stream<I>> tail) {
                this.head = head;
                this.tail = tail;
            }
        }

        public static Stream<Integer> from(int i) {
            return new Cons(i, () -> from(i + 1)); // Wrapping recursive method by a Supplier (Better Approach)
        }

    2)
        TailCall<Integer> from(int i) {
            if(i == 9) return TailCall.getFinalValueContainer(i);
            return TailCall.getSupplierContainer(() -> from(i+1);
        }

        When there is an exit condition inside recursive method, you can wrap lazy call to recursive method with TailCall.
        You cannot do it without exit condition because of the way TailCall is implemented. It will always get SupplierContainer back from recursive method and TailCall doesn't have exit condition.

    (IMP) Rule of Thumb to stop blowing stack for infinite recursion:
    If there is no exit condition inside recursive method, you can wrap recursive call with a Supplier (make it lazy call) and let caller decide the exit condition and calling the method again.
    If there is an exit condition inside recursive method, you can make TailCall as the caller of the recursive method. Basic concept still remains same.

    3) Making method parameters lazy
       Look at Collectors' utility methods that creates a Collector for you.
       It uses a Supplier to send the identity result.
       Stream's collect method uses this identitySupplier.get() for each thread it spawns for parallelism. For each thread, a new identity element is used to aggregate its result.
       This is different than Stream's reduce method that doesn't use supplier for identity result.
                
                    
Spliterator
    Internally, Java uses Spliterator to split the collection/map etc into chunks and based on your choice, it executes them in sequence or parallel.
    You can also create a Stream with your own Spliterator with using StreamSupport.stream(spliterator, parallel=true/false) api.
    Spliterator can directly be retrieved from a collection or from it iterator.


Optional

    Creating Optional object

        Optional<Car> optCar = Optional.empty();
        Optional<Car> optCar = Optional.of(car);   --- If car were null, a NullPointerException would be immediately thrown
        Optional<Car> optCar = Optional.ofNullable(car); --- If car were null, the resulting Optional object would be empty.

    How to start using Optional?

         public class Person {
            private Optional<Car> car;

            public Optional<Car> getCar() {
                return car;
            }
         }
            // or alternatively
         public class Person {
            private Car car;

            public Car getCar() {
                return car;
            }

            public Optional<Car> getCarAsOptional() {   ------- this is how you can add a new method in legacy code and start using Optional as shown below
                return Optional.ofNullable(car);
            }
         }

        public class Car {
            private Optional<Insurance> insurance;

            public Optional<Insurance> getInsurance() {
                return insurance;
            }
        }

        public class Insurance {
            private String name;

            public String getName() {
                return name;
            }
        }

        public String getCarInsuranceName(Person person) {
            String name =
                    Optional.ofNullable(person)
                    .flatMap(p -> p.getCar())  // p.getCar() returns Optional<Car>. So .map method will return Optional<Optional<Car>>. You cannot call getInsurance on Optional<Optional<Car>>. So, use flatMap that returns Optional<Car>.
                    .flatMap(c -> c.getInsurance())
                    .map(i -> i.getName())
                    .orElse("Unknown")
            return name;
        }

    Other methods

        filter, isPresent

            Optional<USB> maybeUSB = ...;
            maybeUSB.filter(usb -> "3.0".equals(usb.getVersion())
                    .ifPresent(() -> System.out.println("ok"));

        get()   --- returns an actual value non-null/null

        orElse(supplier returning default value), orElseGet(supplier returning default value), orElseThrow(supplier throwing an exception)


    COMPREHENSION Pattern

        When you have more than one Optional objects as inputs to some other method/function, then you can use 'Comprehension' pattern to produce an output.

        As a general rule, you should always remember not to use optional.get() or getOrThrow() when using that value as an input to other method/function.
        Instead use comprehension pattern as shown below because it ensures calling getSomething(...) method only if all required inputs to that method are not-null.


           O output = ra.flatMap(a ->
                                    rb.flatMap(b ->
                                                rc.map(c -> getSomething(a, b, c))));

           Optional<O> output = ra.flatMap(a ->
                                              rb.flatMap(b ->
                                                            rc.flatMap(c -> getSomething(a, b, c))));



        You could do
        getSomething(


